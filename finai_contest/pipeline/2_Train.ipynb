{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QMjwq6pS-kFz"
   },
   "source": [
    "# Stock NeurIPS2018 Part 2. Train\n",
    "This series is a reproduction of *the process in the paper Practical Deep Reinforcement Learning Approach for Stock Trading*. \n",
    "\n",
    "This is the second part of the NeurIPS2018 series, introducing how to use FinRL to make data into the gym form environment, and train DRL agents on it.\n",
    "\n",
    "Other demos can be found at the repo of [FinRL-Tutorials]((https://github.com/AI4Finance-Foundation/FinRL-Tutorials))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gT-zXutMgqOS"
   },
   "source": [
    "# Part 1. Install Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "D0vEcPxSJ8hI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/AI4Finance-Foundation/FinRL.git\n",
      "  Cloning https://github.com/AI4Finance-Foundation/FinRL.git to /tmp/pip-req-build-4blykafj\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/AI4Finance-Foundation/FinRL.git /tmp/pip-req-build-4blykafj\n",
      "  Resolved https://github.com/AI4Finance-Foundation/FinRL.git to commit 9e8c38aa5b92bbf0e20f65fc611fd43b43196859\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: alpaca-py<0.38,>=0.37 in /home/feng/CS/market_env/lib/python3.12/site-packages (from finrl==0.3.8) (0.37.0)\n",
      "Requirement already satisfied: alpaca-trade-api<4,>=3 in /home/feng/CS/market_env/lib/python3.12/site-packages (from finrl==0.3.8) (3.2.0)\n",
      "Requirement already satisfied: ccxt<4,>=3 in /home/feng/CS/market_env/lib/python3.12/site-packages (from finrl==0.3.8) (3.1.60)\n",
      "Collecting elegantrl@ git+https://github.com/AI4Finance-Foundation/ElegantRL.git (from finrl==0.3.8)\n",
      "  Cloning https://github.com/AI4Finance-Foundation/ElegantRL.git to /tmp/pip-install-jj2_8qzl/elegantrl_5761d4f0caff4be7ba6c42333970868c\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/AI4Finance-Foundation/ElegantRL.git /tmp/pip-install-jj2_8qzl/elegantrl_5761d4f0caff4be7ba6c42333970868c\n",
      "  Resolved https://github.com/AI4Finance-Foundation/ElegantRL.git to commit 37aac1f592e1add9f9fd37ae8db1094656009b76\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: jqdatasdk<2,>=1 in /home/feng/CS/market_env/lib/python3.12/site-packages (from finrl==0.3.8) (1.9.7)\n",
      "Requirement already satisfied: pandas-market-calendars<6,>=5 in /home/feng/CS/market_env/lib/python3.12/site-packages (from finrl==0.3.8) (5.1.1)\n",
      "Requirement already satisfied: pyfolio-reloaded<0.10,>=0.9 in /home/feng/CS/market_env/lib/python3.12/site-packages (from finrl==0.3.8) (0.9.9)\n",
      "Requirement already satisfied: pyportfolioopt<2,>=1 in /home/feng/CS/market_env/lib/python3.12/site-packages (from finrl==0.3.8) (1.5.6)\n",
      "Requirement already satisfied: ray<3,>=2 in /home/feng/CS/market_env/lib/python3.12/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (2.49.1)\n",
      "Requirement already satisfied: scikit-learn<2,>=1 in /home/feng/CS/market_env/lib/python3.12/site-packages (from finrl==0.3.8) (1.7.2)\n",
      "Requirement already satisfied: selenium<5,>=4 in /home/feng/CS/market_env/lib/python3.12/site-packages (from finrl==0.3.8) (4.32.0)\n",
      "Requirement already satisfied: stable-baselines3>=2.0.0a5 in /home/feng/CS/market_env/lib/python3.12/site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (2.7.0)\n",
      "Requirement already satisfied: stockstats<0.6,>=0.5 in /home/feng/CS/market_env/lib/python3.12/site-packages (from finrl==0.3.8) (0.5.4)\n",
      "Requirement already satisfied: webdriver-manager<5,>=4 in /home/feng/CS/market_env/lib/python3.12/site-packages (from finrl==0.3.8) (4.0.2)\n",
      "Requirement already satisfied: wrds<4,>=3 in /home/feng/CS/market_env/lib/python3.12/site-packages (from finrl==0.3.8) (3.4.0)\n",
      "Requirement already satisfied: yfinance<0.3,>=0.2 in /home/feng/CS/market_env/lib/python3.12/site-packages (from finrl==0.3.8) (0.2.58)\n",
      "Requirement already satisfied: th in /home/feng/CS/market_env/lib/python3.12/site-packages (from elegantrl@ git+https://github.com/AI4Finance-Foundation/ElegantRL.git->finrl==0.3.8) (0.4.1)\n",
      "Requirement already satisfied: numpy in /home/feng/CS/market_env/lib/python3.12/site-packages (from elegantrl@ git+https://github.com/AI4Finance-Foundation/ElegantRL.git->finrl==0.3.8) (2.2.6)\n",
      "Requirement already satisfied: gymnasium in /home/feng/CS/market_env/lib/python3.12/site-packages (from elegantrl@ git+https://github.com/AI4Finance-Foundation/ElegantRL.git->finrl==0.3.8) (1.2.0)\n",
      "Requirement already satisfied: matplotlib in /home/feng/CS/market_env/lib/python3.12/site-packages (from elegantrl@ git+https://github.com/AI4Finance-Foundation/ElegantRL.git->finrl==0.3.8) (3.10.6)\n",
      "Requirement already satisfied: msgpack<2.0.0,>=1.0.3 in /home/feng/CS/market_env/lib/python3.12/site-packages (from alpaca-py<0.38,>=0.37->finrl==0.3.8) (1.0.3)\n",
      "Requirement already satisfied: pandas>=1.5.3 in /home/feng/CS/market_env/lib/python3.12/site-packages (from alpaca-py<0.38,>=0.37->finrl==0.3.8) (2.2.3)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.0.3 in /home/feng/CS/market_env/lib/python3.12/site-packages (from alpaca-py<0.38,>=0.37->finrl==0.3.8) (2.11.9)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.30.0 in /home/feng/CS/market_env/lib/python3.12/site-packages (from alpaca-py<0.38,>=0.37->finrl==0.3.8) (2.32.5)\n",
      "Requirement already satisfied: sseclient-py<2.0.0,>=1.7.2 in /home/feng/CS/market_env/lib/python3.12/site-packages (from alpaca-py<0.38,>=0.37->finrl==0.3.8) (1.8.0)\n",
      "Requirement already satisfied: websockets>=10.4 in /home/feng/CS/market_env/lib/python3.12/site-packages (from alpaca-py<0.38,>=0.37->finrl==0.3.8) (10.4)\n",
      "Requirement already satisfied: urllib3<2,>1.24 in /home/feng/CS/market_env/lib/python3.12/site-packages (from alpaca-trade-api<4,>=3->finrl==0.3.8) (1.26.20)\n",
      "Requirement already satisfied: websocket-client<2,>=0.56.0 in /home/feng/CS/market_env/lib/python3.12/site-packages (from alpaca-trade-api<4,>=3->finrl==0.3.8) (1.8.0)\n",
      "Requirement already satisfied: aiohttp<4,>=3.8.3 in /home/feng/CS/market_env/lib/python3.12/site-packages (from alpaca-trade-api<4,>=3->finrl==0.3.8) (3.12.15)\n",
      "Requirement already satisfied: PyYAML==6.0.1 in /home/feng/CS/market_env/lib/python3.12/site-packages (from alpaca-trade-api<4,>=3->finrl==0.3.8) (6.0.1)\n",
      "Requirement already satisfied: deprecation==2.1.0 in /home/feng/CS/market_env/lib/python3.12/site-packages (from alpaca-trade-api<4,>=3->finrl==0.3.8) (2.1.0)\n",
      "Requirement already satisfied: packaging in /home/feng/CS/market_env/lib/python3.12/site-packages (from deprecation==2.1.0->alpaca-trade-api<4,>=3->finrl==0.3.8) (24.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /home/feng/CS/market_env/lib/python3.12/site-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api<4,>=3->finrl==0.3.8) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /home/feng/CS/market_env/lib/python3.12/site-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api<4,>=3->finrl==0.3.8) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/feng/CS/market_env/lib/python3.12/site-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api<4,>=3->finrl==0.3.8) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/feng/CS/market_env/lib/python3.12/site-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api<4,>=3->finrl==0.3.8) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/feng/CS/market_env/lib/python3.12/site-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api<4,>=3->finrl==0.3.8) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/feng/CS/market_env/lib/python3.12/site-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api<4,>=3->finrl==0.3.8) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/feng/CS/market_env/lib/python3.12/site-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api<4,>=3->finrl==0.3.8) (1.20.1)\n",
      "Requirement already satisfied: setuptools>=60.9.0 in /home/feng/CS/market_env/lib/python3.12/site-packages (from ccxt<4,>=3->finrl==0.3.8) (80.9.0)\n",
      "Requirement already satisfied: certifi>=2018.1.18 in /home/feng/CS/market_env/lib/python3.12/site-packages (from ccxt<4,>=3->finrl==0.3.8) (2025.8.3)\n",
      "Requirement already satisfied: cryptography>=2.6.1 in /home/feng/CS/market_env/lib/python3.12/site-packages (from ccxt<4,>=3->finrl==0.3.8) (45.0.7)\n",
      "Requirement already satisfied: aiodns>=1.1.1 in /home/feng/CS/market_env/lib/python3.12/site-packages (from ccxt<4,>=3->finrl==0.3.8) (3.5.0)\n",
      "Requirement already satisfied: six in /home/feng/CS/market_env/lib/python3.12/site-packages (from jqdatasdk<2,>=1->finrl==0.3.8) (1.17.0)\n",
      "Requirement already satisfied: SQLAlchemy>=1.2.8 in /home/feng/CS/market_env/lib/python3.12/site-packages (from jqdatasdk<2,>=1->finrl==0.3.8) (2.0.43)\n",
      "Requirement already satisfied: pymysql>=0.7.6 in /home/feng/CS/market_env/lib/python3.12/site-packages (from jqdatasdk<2,>=1->finrl==0.3.8) (1.1.2)\n",
      "Requirement already satisfied: thriftpy2!=0.5.1,>=0.3.9 in /home/feng/CS/market_env/lib/python3.12/site-packages (from jqdatasdk<2,>=1->finrl==0.3.8) (0.5.3)\n",
      "Requirement already satisfied: tzdata in /home/feng/CS/market_env/lib/python3.12/site-packages (from pandas-market-calendars<6,>=5->finrl==0.3.8) (2025.2)\n",
      "Requirement already satisfied: python-dateutil in /home/feng/CS/market_env/lib/python3.12/site-packages (from pandas-market-calendars<6,>=5->finrl==0.3.8) (2.9.0.post0)\n",
      "Requirement already satisfied: exchange-calendars>=3.3 in /home/feng/CS/market_env/lib/python3.12/site-packages (from pandas-market-calendars<6,>=5->finrl==0.3.8) (4.11.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/feng/CS/market_env/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.0.3->alpaca-py<0.38,>=0.37->finrl==0.3.8) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /home/feng/CS/market_env/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.0.3->alpaca-py<0.38,>=0.37->finrl==0.3.8) (2.33.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /home/feng/CS/market_env/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.0.3->alpaca-py<0.38,>=0.37->finrl==0.3.8) (4.15.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /home/feng/CS/market_env/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.0.3->alpaca-py<0.38,>=0.37->finrl==0.3.8) (0.4.1)\n",
      "Requirement already satisfied: ipython>=3.2.3 in /home/feng/CS/market_env/lib/python3.12/site-packages (from pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (9.5.0)\n",
      "Requirement already satisfied: pytz>=2014.10 in /home/feng/CS/market_env/lib/python3.12/site-packages (from pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (2025.2)\n",
      "Requirement already satisfied: scipy>=0.14.0 in /home/feng/CS/market_env/lib/python3.12/site-packages (from pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (1.16.2)\n",
      "Requirement already satisfied: seaborn>=0.7.1 in /home/feng/CS/market_env/lib/python3.12/site-packages (from pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (0.13.2)\n",
      "Requirement already satisfied: empyrical-reloaded>=0.5.9 in /home/feng/CS/market_env/lib/python3.12/site-packages (from pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (0.5.12)\n",
      "Requirement already satisfied: cvxpy>=1.1.19 in /home/feng/CS/market_env/lib/python3.12/site-packages (from pyportfolioopt<2,>=1->finrl==0.3.8) (1.7.2)\n",
      "Requirement already satisfied: ecos<3.0.0,>=2.0.14 in /home/feng/CS/market_env/lib/python3.12/site-packages (from pyportfolioopt<2,>=1->finrl==0.3.8) (2.0.14)\n",
      "Requirement already satisfied: plotly<6.0.0,>=5.0.0 in /home/feng/CS/market_env/lib/python3.12/site-packages (from pyportfolioopt<2,>=1->finrl==0.3.8) (5.24.1)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /home/feng/CS/market_env/lib/python3.12/site-packages (from plotly<6.0.0,>=5.0.0->pyportfolioopt<2,>=1->finrl==0.3.8) (9.1.2)\n",
      "Requirement already satisfied: click>=7.0 in /home/feng/CS/market_env/lib/python3.12/site-packages (from ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.8) (8.2.1)\n",
      "Requirement already satisfied: filelock in /home/feng/CS/market_env/lib/python3.12/site-packages (from ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.8) (3.19.1)\n",
      "Requirement already satisfied: jsonschema in /home/feng/CS/market_env/lib/python3.12/site-packages (from ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.8) (4.25.1)\n",
      "Requirement already satisfied: protobuf>=3.20.3 in /home/feng/CS/market_env/lib/python3.12/site-packages (from ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.8) (6.32.1)\n",
      "Requirement already satisfied: aiohttp_cors in /home/feng/CS/market_env/lib/python3.12/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (0.8.1)\n",
      "Requirement already satisfied: colorful in /home/feng/CS/market_env/lib/python3.12/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (0.5.7)\n",
      "Requirement already satisfied: py-spy>=0.4.0 in /home/feng/CS/market_env/lib/python3.12/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (0.4.1)\n",
      "Requirement already satisfied: grpcio>=1.42.0 in /home/feng/CS/market_env/lib/python3.12/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (1.75.0)\n",
      "Requirement already satisfied: opencensus in /home/feng/CS/market_env/lib/python3.12/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (0.11.4)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.30.0 in /home/feng/CS/market_env/lib/python3.12/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (1.37.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-prometheus in /home/feng/CS/market_env/lib/python3.12/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (0.58b0)\n",
      "Requirement already satisfied: opentelemetry-proto in /home/feng/CS/market_env/lib/python3.12/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (1.37.0)\n",
      "Requirement already satisfied: prometheus_client>=0.7.1 in /home/feng/CS/market_env/lib/python3.12/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (0.22.1)\n",
      "Requirement already satisfied: smart_open in /home/feng/CS/market_env/lib/python3.12/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (7.3.1)\n",
      "Requirement already satisfied: virtualenv!=20.21.1,>=20.0.24 in /home/feng/CS/market_env/lib/python3.12/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (20.34.0)\n",
      "Requirement already satisfied: tensorboardX>=1.9 in /home/feng/CS/market_env/lib/python3.12/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (2.6.4)\n",
      "Requirement already satisfied: pyarrow>=9.0.0 in /home/feng/CS/market_env/lib/python3.12/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (21.0.0)\n",
      "Requirement already satisfied: fsspec in /home/feng/CS/market_env/lib/python3.12/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (2025.9.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/feng/CS/market_env/lib/python3.12/site-packages (from requests<3.0.0,>=2.30.0->alpaca-py<0.38,>=0.37->finrl==0.3.8) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/feng/CS/market_env/lib/python3.12/site-packages (from requests<3.0.0,>=2.30.0->alpaca-py<0.38,>=0.37->finrl==0.3.8) (3.10)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/feng/CS/market_env/lib/python3.12/site-packages (from scikit-learn<2,>=1->finrl==0.3.8) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/feng/CS/market_env/lib/python3.12/site-packages (from scikit-learn<2,>=1->finrl==0.3.8) (3.6.0)\n",
      "Requirement already satisfied: trio~=0.17 in /home/feng/CS/market_env/lib/python3.12/site-packages (from selenium<5,>=4->finrl==0.3.8) (0.31.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in /home/feng/CS/market_env/lib/python3.12/site-packages (from selenium<5,>=4->finrl==0.3.8) (0.12.2)\n",
      "Requirement already satisfied: sortedcontainers in /home/feng/CS/market_env/lib/python3.12/site-packages (from trio~=0.17->selenium<5,>=4->finrl==0.3.8) (2.4.0)\n",
      "Requirement already satisfied: outcome in /home/feng/CS/market_env/lib/python3.12/site-packages (from trio~=0.17->selenium<5,>=4->finrl==0.3.8) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in /home/feng/CS/market_env/lib/python3.12/site-packages (from trio~=0.17->selenium<5,>=4->finrl==0.3.8) (1.3.1)\n",
      "Requirement already satisfied: wsproto>=0.14 in /home/feng/CS/market_env/lib/python3.12/site-packages (from trio-websocket~=0.9->selenium<5,>=4->finrl==0.3.8) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in /home/feng/CS/market_env/lib/python3.12/site-packages (from urllib3[socks]<3,>=1.26->selenium<5,>=4->finrl==0.3.8) (1.7.1)\n",
      "Requirement already satisfied: python-dotenv in /home/feng/CS/market_env/lib/python3.12/site-packages (from webdriver-manager<5,>=4->finrl==0.3.8) (1.1.1)\n",
      "Requirement already satisfied: psycopg2-binary<2.10,>=2.9 in /home/feng/CS/market_env/lib/python3.12/site-packages (from wrds<4,>=3->finrl==0.3.8) (2.9.10)\n",
      "Requirement already satisfied: greenlet>=1 in /home/feng/CS/market_env/lib/python3.12/site-packages (from SQLAlchemy>=1.2.8->jqdatasdk<2,>=1->finrl==0.3.8) (3.2.4)\n",
      "Requirement already satisfied: multitasking>=0.0.7 in /home/feng/CS/market_env/lib/python3.12/site-packages (from yfinance<0.3,>=0.2->finrl==0.3.8) (0.0.12)\n",
      "Requirement already satisfied: platformdirs>=2.0.0 in /home/feng/CS/market_env/lib/python3.12/site-packages (from yfinance<0.3,>=0.2->finrl==0.3.8) (4.4.0)\n",
      "Requirement already satisfied: frozendict>=2.3.4 in /home/feng/CS/market_env/lib/python3.12/site-packages (from yfinance<0.3,>=0.2->finrl==0.3.8) (2.4.6)\n",
      "Requirement already satisfied: peewee>=3.16.2 in /home/feng/CS/market_env/lib/python3.12/site-packages (from yfinance<0.3,>=0.2->finrl==0.3.8) (3.17.3)\n",
      "Requirement already satisfied: beautifulsoup4>=4.11.1 in /home/feng/CS/market_env/lib/python3.12/site-packages (from yfinance<0.3,>=0.2->finrl==0.3.8) (4.13.5)\n",
      "Requirement already satisfied: curl_cffi>=0.7 in /home/feng/CS/market_env/lib/python3.12/site-packages (from yfinance<0.3,>=0.2->finrl==0.3.8) (0.13.0)\n",
      "Requirement already satisfied: pycares>=4.9.0 in /home/feng/CS/market_env/lib/python3.12/site-packages (from aiodns>=1.1.1->ccxt<4,>=3->finrl==0.3.8) (4.11.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/feng/CS/market_env/lib/python3.12/site-packages (from beautifulsoup4>=4.11.1->yfinance<0.3,>=0.2->finrl==0.3.8) (2.8)\n",
      "Requirement already satisfied: cffi>=1.14 in /home/feng/CS/market_env/lib/python3.12/site-packages (from cryptography>=2.6.1->ccxt<4,>=3->finrl==0.3.8) (2.0.0)\n",
      "Requirement already satisfied: pycparser in /home/feng/CS/market_env/lib/python3.12/site-packages (from cffi>=1.14->cryptography>=2.6.1->ccxt<4,>=3->finrl==0.3.8) (2.23)\n",
      "Requirement already satisfied: osqp>=0.6.2 in /home/feng/CS/market_env/lib/python3.12/site-packages (from cvxpy>=1.1.19->pyportfolioopt<2,>=1->finrl==0.3.8) (1.0.4)\n",
      "Requirement already satisfied: clarabel>=0.5.0 in /home/feng/CS/market_env/lib/python3.12/site-packages (from cvxpy>=1.1.19->pyportfolioopt<2,>=1->finrl==0.3.8) (0.11.1)\n",
      "Requirement already satisfied: scs>=3.2.4.post1 in /home/feng/CS/market_env/lib/python3.12/site-packages (from cvxpy>=1.1.19->pyportfolioopt<2,>=1->finrl==0.3.8) (3.2.8)\n",
      "Requirement already satisfied: bottleneck>=1.3.0 in /home/feng/CS/market_env/lib/python3.12/site-packages (from empyrical-reloaded>=0.5.9->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (1.6.0)\n",
      "Requirement already satisfied: pyluach in /home/feng/CS/market_env/lib/python3.12/site-packages (from exchange-calendars>=3.3->pandas-market-calendars<6,>=5->finrl==0.3.8) (2.3.0)\n",
      "Requirement already satisfied: toolz in /home/feng/CS/market_env/lib/python3.12/site-packages (from exchange-calendars>=3.3->pandas-market-calendars<6,>=5->finrl==0.3.8) (1.0.0)\n",
      "Requirement already satisfied: korean_lunar_calendar in /home/feng/CS/market_env/lib/python3.12/site-packages (from exchange-calendars>=3.3->pandas-market-calendars<6,>=5->finrl==0.3.8) (0.3.1)\n",
      "Requirement already satisfied: decorator in /home/feng/CS/market_env/lib/python3.12/site-packages (from ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers in /home/feng/CS/market_env/lib/python3.12/site-packages (from ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /home/feng/CS/market_env/lib/python3.12/site-packages (from ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in /home/feng/CS/market_env/lib/python3.12/site-packages (from ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (0.1.7)\n",
      "Requirement already satisfied: pexpect>4.3 in /home/feng/CS/market_env/lib/python3.12/site-packages (from ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /home/feng/CS/market_env/lib/python3.12/site-packages (from ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (3.0.52)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /home/feng/CS/market_env/lib/python3.12/site-packages (from ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (2.19.2)\n",
      "Requirement already satisfied: stack_data in /home/feng/CS/market_env/lib/python3.12/site-packages (from ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (0.6.3)\n",
      "Requirement already satisfied: traitlets>=5.13.0 in /home/feng/CS/market_env/lib/python3.12/site-packages (from ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (5.14.3)\n",
      "Requirement already satisfied: wcwidth in /home/feng/CS/market_env/lib/python3.12/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (0.2.13)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /home/feng/CS/market_env/lib/python3.12/site-packages (from jedi>=0.16->ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (0.8.5)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/feng/CS/market_env/lib/python3.12/site-packages (from matplotlib->elegantrl@ git+https://github.com/AI4Finance-Foundation/ElegantRL.git->finrl==0.3.8) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/feng/CS/market_env/lib/python3.12/site-packages (from matplotlib->elegantrl@ git+https://github.com/AI4Finance-Foundation/ElegantRL.git->finrl==0.3.8) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/feng/CS/market_env/lib/python3.12/site-packages (from matplotlib->elegantrl@ git+https://github.com/AI4Finance-Foundation/ElegantRL.git->finrl==0.3.8) (4.59.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/feng/CS/market_env/lib/python3.12/site-packages (from matplotlib->elegantrl@ git+https://github.com/AI4Finance-Foundation/ElegantRL.git->finrl==0.3.8) (1.4.9)\n",
      "Requirement already satisfied: pillow>=8 in /home/feng/CS/market_env/lib/python3.12/site-packages (from matplotlib->elegantrl@ git+https://github.com/AI4Finance-Foundation/ElegantRL.git->finrl==0.3.8) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/feng/CS/market_env/lib/python3.12/site-packages (from matplotlib->elegantrl@ git+https://github.com/AI4Finance-Foundation/ElegantRL.git->finrl==0.3.8) (3.2.4)\n",
      "Requirement already satisfied: opentelemetry-api==1.37.0 in /home/feng/CS/market_env/lib/python3.12/site-packages (from opentelemetry-sdk>=1.30.0->ray[default,tune]<3,>=2->finrl==0.3.8) (1.37.0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.58b0 in /home/feng/CS/market_env/lib/python3.12/site-packages (from opentelemetry-sdk>=1.30.0->ray[default,tune]<3,>=2->finrl==0.3.8) (0.58b0)\n",
      "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /home/feng/CS/market_env/lib/python3.12/site-packages (from opentelemetry-api==1.37.0->opentelemetry-sdk>=1.30.0->ray[default,tune]<3,>=2->finrl==0.3.8) (8.7.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /home/feng/CS/market_env/lib/python3.12/site-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api==1.37.0->opentelemetry-sdk>=1.30.0->ray[default,tune]<3,>=2->finrl==0.3.8) (3.23.0)\n",
      "Requirement already satisfied: jinja2 in /home/feng/CS/market_env/lib/python3.12/site-packages (from osqp>=0.6.2->cvxpy>=1.1.19->pyportfolioopt<2,>=1->finrl==0.3.8) (3.1.6)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /home/feng/CS/market_env/lib/python3.12/site-packages (from pexpect>4.3->ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (0.7.0)\n",
      "Requirement already satisfied: torch<3.0,>=2.3 in /home/feng/CS/market_env/lib/python3.12/site-packages (from stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (2.8.0)\n",
      "Requirement already satisfied: cloudpickle in /home/feng/CS/market_env/lib/python3.12/site-packages (from stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (3.1.1)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /home/feng/CS/market_env/lib/python3.12/site-packages (from gymnasium->elegantrl@ git+https://github.com/AI4Finance-Foundation/ElegantRL.git->finrl==0.3.8) (0.0.4)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/feng/CS/market_env/lib/python3.12/site-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (1.14.0)\n",
      "Requirement already satisfied: networkx in /home/feng/CS/market_env/lib/python3.12/site-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (3.5)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /home/feng/CS/market_env/lib/python3.12/site-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /home/feng/CS/market_env/lib/python3.12/site-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /home/feng/CS/market_env/lib/python3.12/site-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /home/feng/CS/market_env/lib/python3.12/site-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /home/feng/CS/market_env/lib/python3.12/site-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /home/feng/CS/market_env/lib/python3.12/site-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /home/feng/CS/market_env/lib/python3.12/site-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /home/feng/CS/market_env/lib/python3.12/site-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /home/feng/CS/market_env/lib/python3.12/site-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /home/feng/CS/market_env/lib/python3.12/site-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /home/feng/CS/market_env/lib/python3.12/site-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /home/feng/CS/market_env/lib/python3.12/site-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /home/feng/CS/market_env/lib/python3.12/site-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /home/feng/CS/market_env/lib/python3.12/site-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in /home/feng/CS/market_env/lib/python3.12/site-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (3.4.0)\n",
      "Requirement already satisfied: opencv-python in /home/feng/CS/market_env/lib/python3.12/site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (4.12.0.88)\n",
      "Requirement already satisfied: pygame in /home/feng/CS/market_env/lib/python3.12/site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (2.6.1)\n",
      "Requirement already satisfied: tensorboard>=2.9.1 in /home/feng/CS/market_env/lib/python3.12/site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (2.20.0)\n",
      "Requirement already satisfied: psutil in /home/feng/CS/market_env/lib/python3.12/site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (7.0.0)\n",
      "Requirement already satisfied: tqdm in /home/feng/CS/market_env/lib/python3.12/site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (4.67.1)\n",
      "Requirement already satisfied: rich in /home/feng/CS/market_env/lib/python3.12/site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (14.1.0)\n",
      "Requirement already satisfied: ale-py>=0.9.0 in /home/feng/CS/market_env/lib/python3.12/site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (0.11.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/feng/CS/market_env/lib/python3.12/site-packages (from sympy>=1.13.3->torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (1.3.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in /home/feng/CS/market_env/lib/python3.12/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (2.3.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/feng/CS/market_env/lib/python3.12/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (3.9)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/feng/CS/market_env/lib/python3.12/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/feng/CS/market_env/lib/python3.12/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (3.1.3)\n",
      "Requirement already satisfied: ply<4.0,>=3.4 in /home/feng/CS/market_env/lib/python3.12/site-packages (from thriftpy2!=0.5.1,>=0.3.9->jqdatasdk<2,>=1->finrl==0.3.8) (3.11)\n",
      "Requirement already satisfied: distlib<1,>=0.3.7 in /home/feng/CS/market_env/lib/python3.12/site-packages (from virtualenv!=20.21.1,>=20.0.24->ray[default,tune]<3,>=2->finrl==0.3.8) (0.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/feng/CS/market_env/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (3.0.2)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in /home/feng/CS/market_env/lib/python3.12/site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium<5,>=4->finrl==0.3.8) (0.16.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/feng/CS/market_env/lib/python3.12/site-packages (from jsonschema->ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.8) (2025.9.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /home/feng/CS/market_env/lib/python3.12/site-packages (from jsonschema->ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.8) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/feng/CS/market_env/lib/python3.12/site-packages (from jsonschema->ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.8) (0.27.1)\n",
      "Requirement already satisfied: opencensus-context>=0.1.3 in /home/feng/CS/market_env/lib/python3.12/site-packages (from opencensus->ray[default,tune]<3,>=2->finrl==0.3.8) (0.1.3)\n",
      "Requirement already satisfied: google-api-core<3.0.0,>=1.0.0 in /home/feng/CS/market_env/lib/python3.12/site-packages (from opencensus->ray[default,tune]<3,>=2->finrl==0.3.8) (2.25.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /home/feng/CS/market_env/lib/python3.12/site-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<3,>=2->finrl==0.3.8) (1.70.0)\n",
      "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /home/feng/CS/market_env/lib/python3.12/site-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<3,>=2->finrl==0.3.8) (1.26.1)\n",
      "Requirement already satisfied: google-auth<3.0.0,>=2.14.1 in /home/feng/CS/market_env/lib/python3.12/site-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<3,>=2->finrl==0.3.8) (2.40.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/feng/CS/market_env/lib/python3.12/site-packages (from google-auth<3.0.0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<3,>=2->finrl==0.3.8) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/feng/CS/market_env/lib/python3.12/site-packages (from google-auth<3.0.0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<3,>=2->finrl==0.3.8) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/feng/CS/market_env/lib/python3.12/site-packages (from google-auth<3.0.0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<3,>=2->finrl==0.3.8) (4.9.1)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /home/feng/CS/market_env/lib/python3.12/site-packages (from rsa<5,>=3.1.4->google-auth<3.0.0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<3,>=2->finrl==0.3.8) (0.6.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/feng/CS/market_env/lib/python3.12/site-packages (from rich->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (4.0.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/feng/CS/market_env/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (0.1.2)\n",
      "Requirement already satisfied: wrapt in /home/feng/CS/market_env/lib/python3.12/site-packages (from smart_open->ray[default,tune]<3,>=2->finrl==0.3.8) (1.17.3)\n",
      "Requirement already satisfied: executing>=1.2.0 in /home/feng/CS/market_env/lib/python3.12/site-packages (from stack_data->ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (2.2.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /home/feng/CS/market_env/lib/python3.12/site-packages (from stack_data->ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in /home/feng/CS/market_env/lib/python3.12/site-packages (from stack_data->ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (0.2.3)\n",
      "Requirement already satisfied: niltype<2.0,>=0.3 in /home/feng/CS/market_env/lib/python3.12/site-packages (from th->elegantrl@ git+https://github.com/AI4Finance-Foundation/ElegantRL.git->finrl==0.3.8) (1.0.2)\n"
     ]
    }
   ],
   "source": [
    "## install finrl library\n",
    "!pip install git+https://github.com/AI4Finance-Foundation/FinRL.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "xt1317y2ixSS"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from stable_baselines3.common.logger import configure\n",
    "from finrl.agents.stablebaselines3.models import DRLAgent\n",
    "from finrl.config import INDICATORS, TRAINED_MODEL_DIR, RESULTS_DIR\n",
    "from finrl.main import check_and_make_directories\n",
    "\n",
    "import sys, pathlib\n",
    "sys.path.insert(0, str(pathlib.Path.cwd().parents[1]))\n",
    "from finai_contest.env_stock_trading.env_stock_trading_meta import StockTradingEnv_FinRLMeta\n",
    "from finai_contest.env_stock_trading.env_stock_trading_gym_anytrading import StockTradingEnv_gym_anytrading\n",
    "check_and_make_directories([TRAINED_MODEL_DIR])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aWrSrQv3i0Ng"
   },
   "source": [
    "# Part 2. Build A Market Environment in OpenAI Gym-style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wiHhM2U-XBMZ"
   },
   "source": [
    "![rl_diagram_transparent_bg.png](data:image/png;base64,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)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LeneTRdyZDvy"
   },
   "source": [
    "The core element in reinforcement learning are **agent** and **environment**. You can understand RL as the following process: \n",
    "\n",
    "The agent is active in a world, which is the environment. It observe its current condition as a **state**, and is allowed to do certain **actions**. After the agent execute an action, it will arrive at a new state. At the same time, the environment will have feedback to the agent called **reward**, a numerical signal that tells how good or bad the new state is. As the figure above, agent and environment will keep doing this interaction.\n",
    "\n",
    "The goal of agent is to get as much cumulative reward as possible. Reinforcement learning is the method that agent learns to improve its behavior and achieve that goal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w3H88JXkI93v"
   },
   "source": [
    "To achieve this in Python, we follow the OpenAI gym style to build the stock data into environment.\n",
    "\n",
    "state-action-reward are specified as follows:\n",
    "\n",
    "* **State s**: The state space represents an agent's perception of the market environment. Just like a human trader analyzing various information, here our agent passively observes the price data and technical indicators based on the past data. It will learn by interacting with the market environment (usually by replaying historical data).\n",
    "\n",
    "* **Action a**: The action space includes allowed actions that an agent can take at each state. For example, a  {1, 0, 1}, where 1, 0, 1 represent\n",
    "selling, holding, and buying. When an action operates multiple shares, a {k, ..., 1, 0, 1, ..., k}, e.g.. \"Buy 10 shares of AAPL\" or \"Sell 10 shares of AAPL\" are 10 or 10, respectively\n",
    "\n",
    "* **Reward function r(s, a, s)**: Reward is an incentive for an agent to learn a better policy. For example, it can be the change of the portfolio value when taking a at state s and arriving at new state s',  i.e., r(s, a, s) = v  v, where v and v represent the portfolio values at state s and s, respectively\n",
    "\n",
    "\n",
    "**Market environment**: 30 constituent stocks of Dow Jones Industrial Average (DJIA) index. Accessed at the starting date of the testing period."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SKyZejI0fmp1"
   },
   "source": [
    "## Read data\n",
    "\n",
    "We first read the .csv file of our training data into dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "mFCP1YEhi6oi"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>tic</th>\n",
       "      <th>close</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>open</th>\n",
       "      <th>volume</th>\n",
       "      <th>day</th>\n",
       "      <th>macd</th>\n",
       "      <th>boll_ub</th>\n",
       "      <th>boll_lb</th>\n",
       "      <th>rsi_30</th>\n",
       "      <th>cci_30</th>\n",
       "      <th>dx_30</th>\n",
       "      <th>close_30_sma</th>\n",
       "      <th>close_60_sma</th>\n",
       "      <th>vix</th>\n",
       "      <th>turbulence</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2009-01-02</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>2.724325</td>\n",
       "      <td>2.733032</td>\n",
       "      <td>2.556513</td>\n",
       "      <td>2.578127</td>\n",
       "      <td>7.460152e+08</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.944418</td>\n",
       "      <td>2.619210</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>2.724325</td>\n",
       "      <td>2.724325</td>\n",
       "      <td>39.189999</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2009-01-05</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>2.839303</td>\n",
       "      <td>2.887335</td>\n",
       "      <td>2.783165</td>\n",
       "      <td>2.796975</td>\n",
       "      <td>1.181608e+09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002580</td>\n",
       "      <td>2.944418</td>\n",
       "      <td>2.619210</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>2.781814</td>\n",
       "      <td>2.781814</td>\n",
       "      <td>39.080002</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2009-01-06</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>2.792471</td>\n",
       "      <td>2.917054</td>\n",
       "      <td>2.773558</td>\n",
       "      <td>2.880430</td>\n",
       "      <td>1.289310e+09</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.001835</td>\n",
       "      <td>2.901001</td>\n",
       "      <td>2.669732</td>\n",
       "      <td>70.355335</td>\n",
       "      <td>45.847345</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>2.785366</td>\n",
       "      <td>2.785366</td>\n",
       "      <td>38.560001</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2009-01-07</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>2.732131</td>\n",
       "      <td>2.776861</td>\n",
       "      <td>2.709616</td>\n",
       "      <td>2.756147</td>\n",
       "      <td>7.530488e+08</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-0.000728</td>\n",
       "      <td>2.880447</td>\n",
       "      <td>2.663668</td>\n",
       "      <td>50.429340</td>\n",
       "      <td>-30.767276</td>\n",
       "      <td>43.608078</td>\n",
       "      <td>2.772057</td>\n",
       "      <td>2.772057</td>\n",
       "      <td>43.389999</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2009-01-08</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>2.782865</td>\n",
       "      <td>2.796375</td>\n",
       "      <td>2.703011</td>\n",
       "      <td>2.714720</td>\n",
       "      <td>6.735008e+08</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-0.000086</td>\n",
       "      <td>2.868584</td>\n",
       "      <td>2.679854</td>\n",
       "      <td>60.227100</td>\n",
       "      <td>-8.239200</td>\n",
       "      <td>48.358222</td>\n",
       "      <td>2.774219</td>\n",
       "      <td>2.774219</td>\n",
       "      <td>42.560001</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3268</th>\n",
       "      <td>2021-12-27</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>176.796066</td>\n",
       "      <td>176.884299</td>\n",
       "      <td>173.599958</td>\n",
       "      <td>173.619555</td>\n",
       "      <td>7.491960e+07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.966003</td>\n",
       "      <td>179.988870</td>\n",
       "      <td>156.216790</td>\n",
       "      <td>65.037373</td>\n",
       "      <td>111.207534</td>\n",
       "      <td>48.878538</td>\n",
       "      <td>163.182840</td>\n",
       "      <td>153.508200</td>\n",
       "      <td>17.680000</td>\n",
       "      <td>9.832381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3269</th>\n",
       "      <td>2021-12-28</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>175.776428</td>\n",
       "      <td>177.776459</td>\n",
       "      <td>175.031327</td>\n",
       "      <td>176.629389</td>\n",
       "      <td>7.914430e+07</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.015727</td>\n",
       "      <td>180.195292</td>\n",
       "      <td>157.878032</td>\n",
       "      <td>64.013412</td>\n",
       "      <td>108.220251</td>\n",
       "      <td>50.226129</td>\n",
       "      <td>164.140367</td>\n",
       "      <td>154.110296</td>\n",
       "      <td>17.540001</td>\n",
       "      <td>8.468365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3270</th>\n",
       "      <td>2021-12-29</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>175.864685</td>\n",
       "      <td>177.090189</td>\n",
       "      <td>174.648980</td>\n",
       "      <td>175.815662</td>\n",
       "      <td>6.234890e+07</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.004565</td>\n",
       "      <td>180.775802</td>\n",
       "      <td>158.677932</td>\n",
       "      <td>64.064074</td>\n",
       "      <td>99.807054</td>\n",
       "      <td>48.433600</td>\n",
       "      <td>165.100509</td>\n",
       "      <td>154.771133</td>\n",
       "      <td>16.950001</td>\n",
       "      <td>6.345778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3271</th>\n",
       "      <td>2021-12-30</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>174.707794</td>\n",
       "      <td>177.031359</td>\n",
       "      <td>174.599949</td>\n",
       "      <td>175.952910</td>\n",
       "      <td>5.977300e+07</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.846501</td>\n",
       "      <td>180.938411</td>\n",
       "      <td>159.832003</td>\n",
       "      <td>62.864012</td>\n",
       "      <td>91.887914</td>\n",
       "      <td>48.191208</td>\n",
       "      <td>165.989406</td>\n",
       "      <td>155.380546</td>\n",
       "      <td>17.330000</td>\n",
       "      <td>7.367017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3272</th>\n",
       "      <td>2021-12-31</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>174.090149</td>\n",
       "      <td>175.717606</td>\n",
       "      <td>173.786211</td>\n",
       "      <td>174.599948</td>\n",
       "      <td>6.406230e+07</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.618160</td>\n",
       "      <td>180.652230</td>\n",
       "      <td>161.472117</td>\n",
       "      <td>62.220301</td>\n",
       "      <td>79.373353</td>\n",
       "      <td>44.018826</td>\n",
       "      <td>166.776343</td>\n",
       "      <td>155.965143</td>\n",
       "      <td>17.219999</td>\n",
       "      <td>12.130683</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3273 rows  18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Time   tic       close        high         low        open  \\\n",
       "                                                                         \n",
       "0     2009-01-02  AAPL    2.724325    2.733032    2.556513    2.578127   \n",
       "1     2009-01-05  AAPL    2.839303    2.887335    2.783165    2.796975   \n",
       "2     2009-01-06  AAPL    2.792471    2.917054    2.773558    2.880430   \n",
       "3     2009-01-07  AAPL    2.732131    2.776861    2.709616    2.756147   \n",
       "4     2009-01-08  AAPL    2.782865    2.796375    2.703011    2.714720   \n",
       "...          ...   ...         ...         ...         ...         ...   \n",
       "3268  2021-12-27  AAPL  176.796066  176.884299  173.599958  173.619555   \n",
       "3269  2021-12-28  AAPL  175.776428  177.776459  175.031327  176.629389   \n",
       "3270  2021-12-29  AAPL  175.864685  177.090189  174.648980  175.815662   \n",
       "3271  2021-12-30  AAPL  174.707794  177.031359  174.599949  175.952910   \n",
       "3272  2021-12-31  AAPL  174.090149  175.717606  173.786211  174.599948   \n",
       "\n",
       "            volume  day      macd     boll_ub     boll_lb      rsi_30  \\\n",
       "                                                                        \n",
       "0     7.460152e+08  4.0  0.000000    2.944418    2.619210  100.000000   \n",
       "1     1.181608e+09  0.0  0.002580    2.944418    2.619210  100.000000   \n",
       "2     1.289310e+09  1.0  0.001835    2.901001    2.669732   70.355335   \n",
       "3     7.530488e+08  2.0 -0.000728    2.880447    2.663668   50.429340   \n",
       "4     6.735008e+08  3.0 -0.000086    2.868584    2.679854   60.227100   \n",
       "...            ...  ...       ...         ...         ...         ...   \n",
       "3268  7.491960e+07  0.0  4.966003  179.988870  156.216790   65.037373   \n",
       "3269  7.914430e+07  1.0  5.015727  180.195292  157.878032   64.013412   \n",
       "3270  6.234890e+07  2.0  5.004565  180.775802  158.677932   64.064074   \n",
       "3271  5.977300e+07  3.0  4.846501  180.938411  159.832003   62.864012   \n",
       "3272  6.406230e+07  4.0  4.618160  180.652230  161.472117   62.220301   \n",
       "\n",
       "          cci_30       dx_30  close_30_sma  close_60_sma        vix  \\\n",
       "                                                                      \n",
       "0      66.666667  100.000000      2.724325      2.724325  39.189999   \n",
       "1      66.666667  100.000000      2.781814      2.781814  39.080002   \n",
       "2      45.847345  100.000000      2.785366      2.785366  38.560001   \n",
       "3     -30.767276   43.608078      2.772057      2.772057  43.389999   \n",
       "4      -8.239200   48.358222      2.774219      2.774219  42.560001   \n",
       "...          ...         ...           ...           ...        ...   \n",
       "3268  111.207534   48.878538    163.182840    153.508200  17.680000   \n",
       "3269  108.220251   50.226129    164.140367    154.110296  17.540001   \n",
       "3270   99.807054   48.433600    165.100509    154.771133  16.950001   \n",
       "3271   91.887914   48.191208    165.989406    155.380546  17.330000   \n",
       "3272   79.373353   44.018826    166.776343    155.965143  17.219999   \n",
       "\n",
       "      turbulence  \n",
       "                  \n",
       "0       0.000000  \n",
       "1       0.000000  \n",
       "2       0.000000  \n",
       "3       0.000000  \n",
       "4       0.000000  \n",
       "...          ...  \n",
       "3268    9.832381  \n",
       "3269    8.468365  \n",
       "3270    6.345778  \n",
       "3271    7.367017  \n",
       "3272   12.130683  \n",
       "\n",
       "[3273 rows x 18 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('./data/train_data.csv')\n",
    "# If you are not using the data generated from part 1 of this tutorial, make sure \n",
    "# it has the columns and index in the form that could be make into the environment. \n",
    "# Then you can comment and skip the following two lines.\n",
    "train = train.set_index(train.columns[0])\n",
    "train.index.names = ['']\n",
    "\n",
    "train.rename(columns={train.columns[0]: \"Time\"}, inplace=True)\n",
    "train = train[train[\"tic\"] == \"AAPL\"]\n",
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yw95ZMicgEyi"
   },
   "source": [
    "## Construct the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5WZ6-9q2gq9S"
   },
   "source": [
    "Calculate and specify the parameters we need for constructing the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7T3DZPoaIm8k",
    "outputId": "4817e063-400a-416e-f8f2-4b1c4d9c8408"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stock Dimension: 1, State Space: 11\n"
     ]
    }
   ],
   "source": [
    "stock_dimension = len(train.tic.unique())\n",
    "state_space = 1 + 2*stock_dimension + len(INDICATORS)*stock_dimension\n",
    "print(f\"Stock Dimension: {stock_dimension}, State Space: {state_space}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WsOLoeNcJF8Q"
   },
   "outputs": [],
   "source": [
    "env_used = \"finrl\"\n",
    "# env_used = \"gym_anytrading\"\n",
    "buy_cost_list = sell_cost_list = [0.001] * stock_dimension\n",
    "num_stock_shares = [0] * stock_dimension\n",
    "\n",
    "\n",
    "if env_used ==\"finrl\":\n",
    "    env_kwargs = {\n",
    "        \"hmax\": 100,\n",
    "        \"initial_amount\": 1000000,\n",
    "        \"num_stock_shares\": num_stock_shares,\n",
    "        \"buy_cost_pct\": buy_cost_list,\n",
    "        \"sell_cost_pct\": sell_cost_list,\n",
    "        \"state_space\": state_space,\n",
    "        \"stock_dim\": stock_dimension,\n",
    "        \"tech_indicator_list\": INDICATORS,\n",
    "        \"action_space\": stock_dimension,\n",
    "        \"reward_scaling\": 1e-4\n",
    "    }\n",
    "\n",
    "\n",
    "    e_train_gym = StockTradingEnv_FinRLMeta(df = train, **env_kwargs)\n",
    "\n",
    "elif env_used ==\"gym_anytrading\":\n",
    "    env_kwargs = {\n",
    "    \"hmax\": np.inf,\n",
    "    \"initial_amount\": 1000000,\n",
    "    \"num_stock_shares\": num_stock_shares,\n",
    "    \"buy_cost_pct\": buy_cost_list,\n",
    "    \"sell_cost_pct\": sell_cost_list,\n",
    "    \"state_space\": state_space,\n",
    "    \"stock_dim\": stock_dimension,\n",
    "    \"tech_indicator_list\": INDICATORS,\n",
    "    \"action_space\": 2*stock_dimension,\n",
    "    \"reward_scaling\": 1e-4,\n",
    "    \"window_size\": 30\n",
    "    }\n",
    "\n",
    "    e_train_gym = StockTradingEnv_gym_anytrading(df = train, **env_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7We-q73jjaFQ"
   },
   "source": [
    "## Environment for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aS-SHiGRJK-4",
    "outputId": "a733ecdf-d857-40f5-b399-4325c7ead299"
   },
   "outputs": [],
   "source": [
    "env_train, _ = e_train_gym.get_sb_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_aapl = train[train[\"tic\"] == \"aapl\"]\n",
    "# train_gym_anytrade = pd.DataFrame()\n",
    "# train_gym_anytrade['Time'] = pd.to_datetime(train_aapl['date'])  # Convert to datetime\n",
    "# train_gym_anytrade['Open'] = train_aapl['open']\n",
    "# train_gym_anytrade['High'] = train_aapl['high']\n",
    "# train_gym_anytrade['Low'] = train_aapl['low']\n",
    "# train_gym_anytrade['Close'] = train_aapl['close']\n",
    "# train_gym_anytrade['Volume'] = train_aapl['volume']\n",
    "# train_aapl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gymnasium as gym\n",
    "# import gym_anytrading\n",
    "# env_train = gym.make(\n",
    "#     'stocks-v0',\n",
    "#     df=train_gym_anytrade,\n",
    "#     window_size=30,\n",
    "#     frame_bound=(30, len(train_gym_anytrade))\n",
    "# )\n",
    "# from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "# def get_sb_env(self):\n",
    "#     e = DummyVecEnv([lambda: self])\n",
    "#     obs = e.reset()\n",
    "#     return e, obs\n",
    "# # Patch the method\n",
    "# env_train = env_train.env\n",
    "# env_train = env_train.env\n",
    "# env_train.get_sb_env = get_sb_env.__get__(env_train)\n",
    "# env_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HMNR5nHjh1iz"
   },
   "source": [
    "# Part 3: Train DRL Agents\n",
    "* Here, the DRL algorithms are from **[Stable Baselines 3](https://stable-baselines3.readthedocs.io/en/master/)**. It's a library that implemented popular DRL algorithms using pytorch, succeeding to its old version: Stable Baselines.\n",
    "* Users are also encouraged to try **[ElegantRL](https://github.com/AI4Finance-Foundation/ElegantRL)** and **[Ray RLlib](https://github.com/ray-project/ray)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "364PsqckttcQ"
   },
   "outputs": [],
   "source": [
    "agent = DRLAgent(env = env_train)\n",
    "\n",
    "# Set the corresponding values to 'True' for the algorithms that you want to use\n",
    "if_using_a2c = False\n",
    "if_using_ddpg = False\n",
    "if_using_ppo = True\n",
    "if_using_td3 = False\n",
    "if_using_sac = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YDmqOyF9h1iz"
   },
   "source": [
    "## Agent Training: 5 algorithms (A2C, DDPG, PPO, TD3, SAC)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uijiWgkuh1jB"
   },
   "source": [
    "### Agent 1: A2C\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GUCnkn-HIbmj",
    "outputId": "2794a094-a916-448c-ead1-6e20184dde2a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_steps': 5, 'ent_coef': 0.01, 'learning_rate': 0.0007}\n",
      "Using cuda device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/feng/CS/market_env/lib/python3.12/site-packages/stable_baselines3/common/on_policy_algorithm.py:150: UserWarning: You are trying to run A2C on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "agent = DRLAgent(env = env_train)\n",
    "model_a2c = agent.get_model(\"a2c\")\n",
    "\n",
    "if if_using_a2c:\n",
    "  # set up logger\n",
    "  tmp_path = RESULTS_DIR + '/a2c'\n",
    "  new_logger_a2c = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "  # Set new logger\n",
    "  model_a2c.set_logger(new_logger_a2c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0GVpkWGqH4-D",
    "outputId": "f29cf145-e3b5-4e59-f64d-5921462a8f81"
   },
   "outputs": [],
   "source": [
    "trained_a2c = agent.train_model(model=model_a2c, \n",
    "                             tb_log_name='a2c',\n",
    "                             total_timesteps=50000) if if_using_a2c else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "zjCWfgsg3sVa"
   },
   "outputs": [],
   "source": [
    "trained_a2c.save(TRAINED_MODEL_DIR + \"/agent_a2c\") if if_using_a2c else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MRiOtrywfAo1"
   },
   "source": [
    "### Agent 2: DDPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "M2YadjfnLwgt"
   },
   "outputs": [],
   "source": [
    "# agent = DRLAgent(env = env_train)\n",
    "# model_ddpg = agent.get_model(\"ddpg\")\n",
    "\n",
    "# if if_using_ddpg:\n",
    "#   # set up logger\n",
    "#   tmp_path = RESULTS_DIR + '/ddpg'\n",
    "#   new_logger_ddpg = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "#   # Set new logger\n",
    "#   model_ddpg.set_logger(new_logger_ddpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "tCDa78rqfO_a"
   },
   "outputs": [],
   "source": [
    "trained_ddpg = agent.train_model(model=model_ddpg, \n",
    "                             tb_log_name='ddpg',\n",
    "                             total_timesteps=50000) if if_using_ddpg else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "ne6M2R-WvrUQ"
   },
   "outputs": [],
   "source": [
    "trained_ddpg.save(TRAINED_MODEL_DIR + \"/agent_ddpg\") if if_using_ddpg else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_gDkU-j-fCmZ"
   },
   "source": [
    "### Agent 3: PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "y5D5PFUhMzSV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_steps': 2048, 'ent_coef': 0.01, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cuda device\n",
      "Logging to results/ppo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/feng/CS/market_env/lib/python3.12/site-packages/stable_baselines3/common/on_policy_algorithm.py:150: UserWarning: You are trying to run PPO on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "agent = DRLAgent(env = env_train)\n",
    "PPO_PARAMS = {\n",
    "    \"n_steps\": 2048,\n",
    "    \"ent_coef\": 0.01,\n",
    "    \"learning_rate\": 0.00025,\n",
    "    \"batch_size\": 128,\n",
    "}\n",
    "model_ppo = agent.get_model(\"ppo\",model_kwargs = PPO_PARAMS)\n",
    "\n",
    "if if_using_ppo:\n",
    "  # set up logger\n",
    "  tmp_path = RESULTS_DIR + '/ppo'\n",
    "  new_logger_ppo = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "  # Set new logger\n",
    "  model_ppo.set_logger(new_logger_ppo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "Gt8eIQKYM4G3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "| time/              |             |\n",
      "|    fps             | 441         |\n",
      "|    iterations      | 1           |\n",
      "|    time_elapsed    | 4           |\n",
      "|    total_timesteps | 2048        |\n",
      "| train/             |             |\n",
      "|    reward          | -0.16084698 |\n",
      "|    reward_max      | 11.360079   |\n",
      "|    reward_mean     | 0.029633017 |\n",
      "|    reward_min      | -10.666002  |\n",
      "------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 428         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 9           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009632603 |\n",
      "|    clip_fraction        | 0.031       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.687      |\n",
      "|    explained_variance   | -0.0236     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 9.15        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00615    |\n",
      "|    reward               | 0.0         |\n",
      "|    reward_max           | 13.556941   |\n",
      "|    reward_mean          | 0.059533425 |\n",
      "|    reward_min           | -11.197764  |\n",
      "|    value_loss           | 20          |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 440        |\n",
      "|    iterations           | 3          |\n",
      "|    time_elapsed         | 13         |\n",
      "|    total_timesteps      | 6144       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00891707 |\n",
      "|    clip_fraction        | 0.00903    |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.674     |\n",
      "|    explained_variance   | -0.00338   |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 17.1       |\n",
      "|    n_updates            | 20         |\n",
      "|    policy_gradient_loss | -0.00417   |\n",
      "|    reward               | 6.2807097  |\n",
      "|    reward_max           | 49.70007   |\n",
      "|    reward_mean          | 0.13625112 |\n",
      "|    reward_min           | -37.842068 |\n",
      "|    value_loss           | 39.5       |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 444          |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 18           |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016016548 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.662       |\n",
      "|    explained_variance   | -0.000373    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 93.7         |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.00233     |\n",
      "|    reward               | -0.32241926  |\n",
      "|    reward_max           | 58.34453     |\n",
      "|    reward_mean          | 0.3207479    |\n",
      "|    reward_min           | -43.82483    |\n",
      "|    value_loss           | 169          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 447         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 22          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.000646818 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.65       |\n",
      "|    explained_variance   | -0.00133    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 181         |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.00163    |\n",
      "|    reward               | -0.13768096 |\n",
      "|    reward_max           | 51.046013   |\n",
      "|    reward_mean          | 0.1797505   |\n",
      "|    reward_min           | -49.497547  |\n",
      "|    value_loss           | 352         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 447          |\n",
      "|    iterations           | 6            |\n",
      "|    time_elapsed         | 27           |\n",
      "|    total_timesteps      | 12288        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016888652 |\n",
      "|    clip_fraction        | 9.77e-05     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.651       |\n",
      "|    explained_variance   | 0.00252      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 113          |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.00219     |\n",
      "|    reward               | -0.14594254  |\n",
      "|    reward_max           | 8.703979     |\n",
      "|    reward_mean          | 0.0040309466 |\n",
      "|    reward_min           | -16.02745    |\n",
      "|    value_loss           | 219          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 447          |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 32           |\n",
      "|    total_timesteps      | 14336        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034637558 |\n",
      "|    clip_fraction        | 0.00845      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.66        |\n",
      "|    explained_variance   | -0.0978      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 9.58         |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | -0.00633     |\n",
      "|    reward               | -0.983309    |\n",
      "|    reward_max           | 16.021606    |\n",
      "|    reward_mean          | 0.019139241  |\n",
      "|    reward_min           | -17.49452    |\n",
      "|    value_loss           | 21.4         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 445          |\n",
      "|    iterations           | 8            |\n",
      "|    time_elapsed         | 36           |\n",
      "|    total_timesteps      | 16384        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046443716 |\n",
      "|    clip_fraction        | 0.0256       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.662       |\n",
      "|    explained_variance   | -0.012       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 16.6         |\n",
      "|    n_updates            | 70           |\n",
      "|    policy_gradient_loss | -0.00697     |\n",
      "|    reward               | 0.0          |\n",
      "|    reward_max           | 23.342108    |\n",
      "|    reward_mean          | 0.08742529   |\n",
      "|    reward_min           | -24.27776    |\n",
      "|    value_loss           | 33.1         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 446          |\n",
      "|    iterations           | 9            |\n",
      "|    time_elapsed         | 41           |\n",
      "|    total_timesteps      | 18432        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0024207733 |\n",
      "|    clip_fraction        | 0.00415      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.656       |\n",
      "|    explained_variance   | -0.00116     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 21.8         |\n",
      "|    n_updates            | 80           |\n",
      "|    policy_gradient_loss | -0.00633     |\n",
      "|    reward               | -1.6732575   |\n",
      "|    reward_max           | 12.962628    |\n",
      "|    reward_mean          | 0.012568929  |\n",
      "|    reward_min           | -19.571505   |\n",
      "|    value_loss           | 42.7         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 448          |\n",
      "|    iterations           | 10           |\n",
      "|    time_elapsed         | 45           |\n",
      "|    total_timesteps      | 20480        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037759317 |\n",
      "|    clip_fraction        | 0.0248       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.64        |\n",
      "|    explained_variance   | -0.0213      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 24.9         |\n",
      "|    n_updates            | 90           |\n",
      "|    policy_gradient_loss | -0.0073      |\n",
      "|    reward               | 0.0          |\n",
      "|    reward_max           | 13.196852    |\n",
      "|    reward_mean          | 0.056071006  |\n",
      "|    reward_min           | -15.25714    |\n",
      "|    value_loss           | 33.7         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 449         |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 50          |\n",
      "|    total_timesteps      | 22528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005466357 |\n",
      "|    clip_fraction        | 0.0222      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.629      |\n",
      "|    explained_variance   | -0.0128     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 19          |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.00666    |\n",
      "|    reward               | -0.3882997  |\n",
      "|    reward_max           | 28.661993   |\n",
      "|    reward_mean          | 0.11473374  |\n",
      "|    reward_min           | -46.44565   |\n",
      "|    value_loss           | 36.9        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 449         |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 54          |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003571578 |\n",
      "|    clip_fraction        | 0.00635     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.627      |\n",
      "|    explained_variance   | 0.00392     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 23.8        |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.00647    |\n",
      "|    reward               | 1.8688581   |\n",
      "|    reward_max           | 41.591057   |\n",
      "|    reward_mean          | 0.24416915  |\n",
      "|    reward_min           | -39.702847  |\n",
      "|    value_loss           | 58.4        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 451          |\n",
      "|    iterations           | 13           |\n",
      "|    time_elapsed         | 59           |\n",
      "|    total_timesteps      | 26624        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018088138 |\n",
      "|    clip_fraction        | 0.000439     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.613       |\n",
      "|    explained_variance   | -0.00752     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 82.7         |\n",
      "|    n_updates            | 120          |\n",
      "|    policy_gradient_loss | -0.00356     |\n",
      "|    reward               | 0.0          |\n",
      "|    reward_max           | 32.815643    |\n",
      "|    reward_mean          | 0.051196367  |\n",
      "|    reward_min           | -37.370644   |\n",
      "|    value_loss           | 152          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 451          |\n",
      "|    iterations           | 14           |\n",
      "|    time_elapsed         | 63           |\n",
      "|    total_timesteps      | 28672        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0043141264 |\n",
      "|    clip_fraction        | 0.0155       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.619       |\n",
      "|    explained_variance   | 0.000213     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 71.5         |\n",
      "|    n_updates            | 130          |\n",
      "|    policy_gradient_loss | -0.0058      |\n",
      "|    reward               | -0.14549382  |\n",
      "|    reward_max           | 12.279348    |\n",
      "|    reward_mean          | 0.013523798  |\n",
      "|    reward_min           | -12.478744   |\n",
      "|    value_loss           | 140          |\n",
      "------------------------------------------\n",
      "day: 3272, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset:   3145126.40\n",
      "total_reward:      2145126.40\n",
      "total_cost:        2094256.49\n",
      "total_trades:      1506\n",
      "Sharpe: 0.503\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 453         |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 67          |\n",
      "|    total_timesteps      | 30720       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005766706 |\n",
      "|    clip_fraction        | 0.0288      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.615      |\n",
      "|    explained_variance   | -0.0181     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 9.69        |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.00912    |\n",
      "|    reward               | -0.21609043 |\n",
      "|    reward_max           | 23.515583   |\n",
      "|    reward_mean          | 0.13902509  |\n",
      "|    reward_min           | -25.56539   |\n",
      "|    value_loss           | 18.9        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 455          |\n",
      "|    iterations           | 16           |\n",
      "|    time_elapsed         | 71           |\n",
      "|    total_timesteps      | 32768        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046236156 |\n",
      "|    clip_fraction        | 0.0196       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.599       |\n",
      "|    explained_variance   | -0.00617     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 24           |\n",
      "|    n_updates            | 150          |\n",
      "|    policy_gradient_loss | -0.00777     |\n",
      "|    reward               | 0.12711044   |\n",
      "|    reward_max           | 39.865307    |\n",
      "|    reward_mean          | 0.1029033    |\n",
      "|    reward_min           | -38.70718    |\n",
      "|    value_loss           | 64.4         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 456         |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 76          |\n",
      "|    total_timesteps      | 34816       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004860512 |\n",
      "|    clip_fraction        | 0.0179      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.578      |\n",
      "|    explained_variance   | -0.00785    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 92.3        |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.00561    |\n",
      "|    reward               | -1.0528301  |\n",
      "|    reward_max           | 19.076067   |\n",
      "|    reward_mean          | 0.06903628  |\n",
      "|    reward_min           | -17.256868  |\n",
      "|    value_loss           | 212         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 457          |\n",
      "|    iterations           | 18           |\n",
      "|    time_elapsed         | 80           |\n",
      "|    total_timesteps      | 36864        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039048304 |\n",
      "|    clip_fraction        | 0.0208       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.586       |\n",
      "|    explained_variance   | 0.00802      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 27.8         |\n",
      "|    n_updates            | 170          |\n",
      "|    policy_gradient_loss | -0.00874     |\n",
      "|    reward               | -0.13438943  |\n",
      "|    reward_max           | 31.332272    |\n",
      "|    reward_mean          | 0.05598103   |\n",
      "|    reward_min           | -37.674503   |\n",
      "|    value_loss           | 56.2         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 455          |\n",
      "|    iterations           | 19           |\n",
      "|    time_elapsed         | 85           |\n",
      "|    total_timesteps      | 38912        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.002047285  |\n",
      "|    clip_fraction        | 0.00659      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.583       |\n",
      "|    explained_variance   | -0.0134      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 42.5         |\n",
      "|    n_updates            | 180          |\n",
      "|    policy_gradient_loss | -0.00484     |\n",
      "|    reward               | 0.0          |\n",
      "|    reward_max           | 9.084628     |\n",
      "|    reward_mean          | -0.003106627 |\n",
      "|    reward_min           | -13.212106   |\n",
      "|    value_loss           | 78.5         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 455         |\n",
      "|    iterations           | 20          |\n",
      "|    time_elapsed         | 90          |\n",
      "|    total_timesteps      | 40960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005541573 |\n",
      "|    clip_fraction        | 0.0253      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.582      |\n",
      "|    explained_variance   | -0.0172     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 14.2        |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.00885    |\n",
      "|    reward               | 3.9064555   |\n",
      "|    reward_max           | 20.795504   |\n",
      "|    reward_mean          | 0.08607967  |\n",
      "|    reward_min           | -15.598019  |\n",
      "|    value_loss           | 24.9        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 455         |\n",
      "|    iterations           | 21          |\n",
      "|    time_elapsed         | 94          |\n",
      "|    total_timesteps      | 43008       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003705018 |\n",
      "|    clip_fraction        | 0.0191      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.571      |\n",
      "|    explained_variance   | 0.0032      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 46.3        |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.00729    |\n",
      "|    reward               | -2.1633565  |\n",
      "|    reward_max           | 55.685238   |\n",
      "|    reward_mean          | 0.291779    |\n",
      "|    reward_min           | -48.010197  |\n",
      "|    value_loss           | 92.3        |\n",
      "-----------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 455           |\n",
      "|    iterations           | 22            |\n",
      "|    time_elapsed         | 98            |\n",
      "|    total_timesteps      | 45056         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00086173054 |\n",
      "|    clip_fraction        | 0.00259       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.567        |\n",
      "|    explained_variance   | 0.00484       |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 120           |\n",
      "|    n_updates            | 210           |\n",
      "|    policy_gradient_loss | -0.00189      |\n",
      "|    reward               | 4.0839553     |\n",
      "|    reward_max           | 23.786276     |\n",
      "|    reward_mean          | 0.05716871    |\n",
      "|    reward_min           | -28.68981     |\n",
      "|    value_loss           | 337           |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 456          |\n",
      "|    iterations           | 23           |\n",
      "|    time_elapsed         | 103          |\n",
      "|    total_timesteps      | 47104        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039684307 |\n",
      "|    clip_fraction        | 0.0189       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.564       |\n",
      "|    explained_variance   | -0.00528     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 65           |\n",
      "|    n_updates            | 220          |\n",
      "|    policy_gradient_loss | -0.00638     |\n",
      "|    reward               | 0.80035853   |\n",
      "|    reward_max           | 52.880295    |\n",
      "|    reward_mean          | 0.15862888   |\n",
      "|    reward_min           | -58.2524     |\n",
      "|    value_loss           | 119          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 457          |\n",
      "|    iterations           | 24           |\n",
      "|    time_elapsed         | 107          |\n",
      "|    total_timesteps      | 49152        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023082353 |\n",
      "|    clip_fraction        | 0.00601      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.57        |\n",
      "|    explained_variance   | 0.00027      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 152          |\n",
      "|    n_updates            | 230          |\n",
      "|    policy_gradient_loss | -0.00398     |\n",
      "|    reward               | 3.5879583    |\n",
      "|    reward_max           | 21.380718    |\n",
      "|    reward_mean          | 0.08002767   |\n",
      "|    reward_min           | -22.058159   |\n",
      "|    value_loss           | 287          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 458          |\n",
      "|    iterations           | 25           |\n",
      "|    time_elapsed         | 111          |\n",
      "|    total_timesteps      | 51200        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046944255 |\n",
      "|    clip_fraction        | 0.0408       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.548       |\n",
      "|    explained_variance   | -0.00969     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 19.5         |\n",
      "|    n_updates            | 240          |\n",
      "|    policy_gradient_loss | -0.00975     |\n",
      "|    reward               | -0.39789468  |\n",
      "|    reward_max           | 11.109412    |\n",
      "|    reward_mean          | 0.016831176  |\n",
      "|    reward_min           | -10.837145   |\n",
      "|    value_loss           | 52.5         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 460          |\n",
      "|    iterations           | 26           |\n",
      "|    time_elapsed         | 115          |\n",
      "|    total_timesteps      | 53248        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035336814 |\n",
      "|    clip_fraction        | 0.0164       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.554       |\n",
      "|    explained_variance   | -0.0212      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 16.8         |\n",
      "|    n_updates            | 250          |\n",
      "|    policy_gradient_loss | -0.00731     |\n",
      "|    reward               | 7.9402456    |\n",
      "|    reward_max           | 28.592127    |\n",
      "|    reward_mean          | 0.2397636    |\n",
      "|    reward_min           | -20.837458   |\n",
      "|    value_loss           | 30.8         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 460          |\n",
      "|    iterations           | 27           |\n",
      "|    time_elapsed         | 120          |\n",
      "|    total_timesteps      | 55296        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022176607 |\n",
      "|    clip_fraction        | 0.00459      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.56        |\n",
      "|    explained_variance   | 0.00433      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 40.4         |\n",
      "|    n_updates            | 260          |\n",
      "|    policy_gradient_loss | -0.00449     |\n",
      "|    reward               | 10.582605    |\n",
      "|    reward_max           | 26.036789    |\n",
      "|    reward_mean          | 0.0805953    |\n",
      "|    reward_min           | -43.62246    |\n",
      "|    value_loss           | 98.5         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 461          |\n",
      "|    iterations           | 28           |\n",
      "|    time_elapsed         | 124          |\n",
      "|    total_timesteps      | 57344        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0051758476 |\n",
      "|    clip_fraction        | 0.0293       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.541       |\n",
      "|    explained_variance   | -0.0103      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 109          |\n",
      "|    n_updates            | 270          |\n",
      "|    policy_gradient_loss | -0.00631     |\n",
      "|    reward               | 0.0          |\n",
      "|    reward_max           | 25.869408    |\n",
      "|    reward_mean          | 0.05158448   |\n",
      "|    reward_min           | -37.619415   |\n",
      "|    value_loss           | 216          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 461          |\n",
      "|    iterations           | 29           |\n",
      "|    time_elapsed         | 128          |\n",
      "|    total_timesteps      | 59392        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045477534 |\n",
      "|    clip_fraction        | 0.0274       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.521       |\n",
      "|    explained_variance   | 0.00236      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 50           |\n",
      "|    n_updates            | 280          |\n",
      "|    policy_gradient_loss | -0.00802     |\n",
      "|    reward               | -0.21527225  |\n",
      "|    reward_max           | 57.175602    |\n",
      "|    reward_mean          | 0.27931878   |\n",
      "|    reward_min           | -53.52757    |\n",
      "|    value_loss           | 124          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 462          |\n",
      "|    iterations           | 30           |\n",
      "|    time_elapsed         | 132          |\n",
      "|    total_timesteps      | 61440        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031763627 |\n",
      "|    clip_fraction        | 0.0117       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.502       |\n",
      "|    explained_variance   | 0.00127      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 191          |\n",
      "|    n_updates            | 290          |\n",
      "|    policy_gradient_loss | -0.00339     |\n",
      "|    reward               | 3.8521008    |\n",
      "|    reward_max           | 25.242758    |\n",
      "|    reward_mean          | 0.05083802   |\n",
      "|    reward_min           | -32.39593    |\n",
      "|    value_loss           | 313          |\n",
      "------------------------------------------\n",
      "day: 3272, episode: 20\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset:   4579766.70\n",
      "total_reward:      3579766.70\n",
      "total_cost:        3689106.65\n",
      "total_trades:      1191\n",
      "Sharpe: 0.600\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 461          |\n",
      "|    iterations           | 31           |\n",
      "|    time_elapsed         | 137          |\n",
      "|    total_timesteps      | 63488        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030492877 |\n",
      "|    clip_fraction        | 0.0141       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.494       |\n",
      "|    explained_variance   | -0.00379     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 74.8         |\n",
      "|    n_updates            | 300          |\n",
      "|    policy_gradient_loss | -0.00604     |\n",
      "|    reward               | -0.8526972   |\n",
      "|    reward_max           | 42.491573    |\n",
      "|    reward_mean          | 0.15538926   |\n",
      "|    reward_min           | -51.779694   |\n",
      "|    value_loss           | 139          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 461          |\n",
      "|    iterations           | 32           |\n",
      "|    time_elapsed         | 141          |\n",
      "|    total_timesteps      | 65536        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014822875 |\n",
      "|    clip_fraction        | 0.00215      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.49        |\n",
      "|    explained_variance   | -0.00628     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 113          |\n",
      "|    n_updates            | 310          |\n",
      "|    policy_gradient_loss | -0.00416     |\n",
      "|    reward               | 2.9028878    |\n",
      "|    reward_max           | 57.648567    |\n",
      "|    reward_mean          | 0.2225981    |\n",
      "|    reward_min           | -69.901146   |\n",
      "|    value_loss           | 278          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 461          |\n",
      "|    iterations           | 33           |\n",
      "|    time_elapsed         | 146          |\n",
      "|    total_timesteps      | 67584        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025851699 |\n",
      "|    clip_fraction        | 0.0111       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.484       |\n",
      "|    explained_variance   | -0.00226     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 274          |\n",
      "|    n_updates            | 320          |\n",
      "|    policy_gradient_loss | -0.00566     |\n",
      "|    reward               | -4.1924486   |\n",
      "|    reward_max           | 14.445266    |\n",
      "|    reward_mean          | 0.0379277    |\n",
      "|    reward_min           | -19.214716   |\n",
      "|    value_loss           | 384          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 459          |\n",
      "|    iterations           | 34           |\n",
      "|    time_elapsed         | 151          |\n",
      "|    total_timesteps      | 69632        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0047090547 |\n",
      "|    clip_fraction        | 0.0331       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.493       |\n",
      "|    explained_variance   | -0.00643     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 18.3         |\n",
      "|    n_updates            | 330          |\n",
      "|    policy_gradient_loss | -0.00945     |\n",
      "|    reward               | 4.374795     |\n",
      "|    reward_max           | 38.29404     |\n",
      "|    reward_mean          | 0.19232462   |\n",
      "|    reward_min           | -44.021835   |\n",
      "|    value_loss           | 40.1         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 460          |\n",
      "|    iterations           | 35           |\n",
      "|    time_elapsed         | 155          |\n",
      "|    total_timesteps      | 71680        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0040625604 |\n",
      "|    clip_fraction        | 0.0406       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.46        |\n",
      "|    explained_variance   | -0.00406     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 90.3         |\n",
      "|    n_updates            | 340          |\n",
      "|    policy_gradient_loss | -0.00783     |\n",
      "|    reward               | -0.38591447  |\n",
      "|    reward_max           | 38.22956     |\n",
      "|    reward_mean          | 0.06665179   |\n",
      "|    reward_min           | -45.967934   |\n",
      "|    value_loss           | 155          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 461          |\n",
      "|    iterations           | 36           |\n",
      "|    time_elapsed         | 159          |\n",
      "|    total_timesteps      | 73728        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023363694 |\n",
      "|    clip_fraction        | 0.0133       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.477       |\n",
      "|    explained_variance   | -0.00102     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 68.2         |\n",
      "|    n_updates            | 350          |\n",
      "|    policy_gradient_loss | -0.00574     |\n",
      "|    reward               | -0.80093426  |\n",
      "|    reward_max           | 27.213684    |\n",
      "|    reward_mean          | 0.116147935  |\n",
      "|    reward_min           | -28.44622    |\n",
      "|    value_loss           | 189          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 461          |\n",
      "|    iterations           | 37           |\n",
      "|    time_elapsed         | 164          |\n",
      "|    total_timesteps      | 75776        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034301034 |\n",
      "|    clip_fraction        | 0.0316       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.471       |\n",
      "|    explained_variance   | -0.000311    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 56.9         |\n",
      "|    n_updates            | 360          |\n",
      "|    policy_gradient_loss | -0.00821     |\n",
      "|    reward               | 0.65527517   |\n",
      "|    reward_max           | 20.154774    |\n",
      "|    reward_mean          | 0.10237266   |\n",
      "|    reward_min           | -24.234474   |\n",
      "|    value_loss           | 101          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 461          |\n",
      "|    iterations           | 38           |\n",
      "|    time_elapsed         | 168          |\n",
      "|    total_timesteps      | 77824        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042395378 |\n",
      "|    clip_fraction        | 0.0327       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.468       |\n",
      "|    explained_variance   | 0.00319      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 32           |\n",
      "|    n_updates            | 370          |\n",
      "|    policy_gradient_loss | -0.0118      |\n",
      "|    reward               | -1.6722625   |\n",
      "|    reward_max           | 31.510796    |\n",
      "|    reward_mean          | 0.087308705  |\n",
      "|    reward_min           | -39.638935   |\n",
      "|    value_loss           | 54.2         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 461          |\n",
      "|    iterations           | 39           |\n",
      "|    time_elapsed         | 172          |\n",
      "|    total_timesteps      | 79872        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0020057773 |\n",
      "|    clip_fraction        | 0.00811      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.472       |\n",
      "|    explained_variance   | 0.000252     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 104          |\n",
      "|    n_updates            | 380          |\n",
      "|    policy_gradient_loss | -0.00498     |\n",
      "|    reward               | -0.431447    |\n",
      "|    reward_max           | 53.944298    |\n",
      "|    reward_mean          | 0.3494957    |\n",
      "|    reward_min           | -64.863625   |\n",
      "|    value_loss           | 212          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 461          |\n",
      "|    iterations           | 40           |\n",
      "|    time_elapsed         | 177          |\n",
      "|    total_timesteps      | 81920        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0043448852 |\n",
      "|    clip_fraction        | 0.0269       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.454       |\n",
      "|    explained_variance   | -0.00497     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 245          |\n",
      "|    n_updates            | 390          |\n",
      "|    policy_gradient_loss | -0.005       |\n",
      "|    reward               | -0.13365093  |\n",
      "|    reward_max           | 118.295494   |\n",
      "|    reward_mean          | 0.7525994    |\n",
      "|    reward_min           | -130.61632   |\n",
      "|    value_loss           | 443          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 461          |\n",
      "|    iterations           | 41           |\n",
      "|    time_elapsed         | 182          |\n",
      "|    total_timesteps      | 83968        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 9.656104e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.443       |\n",
      "|    explained_variance   | -0.00233     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 729          |\n",
      "|    n_updates            | 400          |\n",
      "|    policy_gradient_loss | -0.00118     |\n",
      "|    reward               | 1.4637164    |\n",
      "|    reward_max           | 26.978058    |\n",
      "|    reward_mean          | 0.07571043   |\n",
      "|    reward_min           | -34.2575     |\n",
      "|    value_loss           | 1.85e+03     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 461          |\n",
      "|    iterations           | 42           |\n",
      "|    time_elapsed         | 186          |\n",
      "|    total_timesteps      | 86016        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028340681 |\n",
      "|    clip_fraction        | 0.0223       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.461       |\n",
      "|    explained_variance   | 0.00917      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 61.6         |\n",
      "|    n_updates            | 410          |\n",
      "|    policy_gradient_loss | -0.00605     |\n",
      "|    reward               | -4.4428835   |\n",
      "|    reward_max           | 70.774315    |\n",
      "|    reward_mean          | 0.5357658    |\n",
      "|    reward_min           | -81.94479    |\n",
      "|    value_loss           | 119          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 459         |\n",
      "|    iterations           | 43          |\n",
      "|    time_elapsed         | 191         |\n",
      "|    total_timesteps      | 88064       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002978871 |\n",
      "|    clip_fraction        | 0.0062      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.438      |\n",
      "|    explained_variance   | -0.00388    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 322         |\n",
      "|    n_updates            | 420         |\n",
      "|    policy_gradient_loss | -0.00404    |\n",
      "|    reward               | 0.65138245  |\n",
      "|    reward_max           | 82.232704   |\n",
      "|    reward_mean          | 0.19786462  |\n",
      "|    reward_min           | -84.45893   |\n",
      "|    value_loss           | 443         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 459          |\n",
      "|    iterations           | 44           |\n",
      "|    time_elapsed         | 196          |\n",
      "|    total_timesteps      | 90112        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0021852662 |\n",
      "|    clip_fraction        | 0.0101       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.447       |\n",
      "|    explained_variance   | 0.00419      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 410          |\n",
      "|    n_updates            | 430          |\n",
      "|    policy_gradient_loss | -0.00519     |\n",
      "|    reward               | -2.4608355   |\n",
      "|    reward_max           | 47.804       |\n",
      "|    reward_mean          | 0.1875954    |\n",
      "|    reward_min           | -50.814884   |\n",
      "|    value_loss           | 738          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 459         |\n",
      "|    iterations           | 45          |\n",
      "|    time_elapsed         | 200         |\n",
      "|    total_timesteps      | 92160       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002622412 |\n",
      "|    clip_fraction        | 0.0215      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.448      |\n",
      "|    explained_variance   | 0.0103      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 78.1        |\n",
      "|    n_updates            | 440         |\n",
      "|    policy_gradient_loss | -0.00678    |\n",
      "|    reward               | -4.126679   |\n",
      "|    reward_max           | 48.886135   |\n",
      "|    reward_mean          | 0.21251906  |\n",
      "|    reward_min           | -46.627487  |\n",
      "|    value_loss           | 230         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 459          |\n",
      "|    iterations           | 46           |\n",
      "|    time_elapsed         | 205          |\n",
      "|    total_timesteps      | 94208        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032727055 |\n",
      "|    clip_fraction        | 0.0215       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.438       |\n",
      "|    explained_variance   | -0.0124      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 134          |\n",
      "|    n_updates            | 450          |\n",
      "|    policy_gradient_loss | -0.00772     |\n",
      "|    reward               | 1.2306552    |\n",
      "|    reward_max           | 21.627821    |\n",
      "|    reward_mean          | 0.027662938  |\n",
      "|    reward_min           | -29.922499   |\n",
      "|    value_loss           | 199          |\n",
      "------------------------------------------\n",
      "day: 3272, episode: 30\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset:   6356363.99\n",
      "total_reward:      5356363.99\n",
      "total_cost:        2989013.22\n",
      "total_trades:      1085\n",
      "Sharpe: 0.691\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 458          |\n",
      "|    iterations           | 47           |\n",
      "|    time_elapsed         | 209          |\n",
      "|    total_timesteps      | 96256        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028206452 |\n",
      "|    clip_fraction        | 0.0239       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.452       |\n",
      "|    explained_variance   | -0.00898     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 77.3         |\n",
      "|    n_updates            | 460          |\n",
      "|    policy_gradient_loss | -0.00558     |\n",
      "|    reward               | -0.21992996  |\n",
      "|    reward_max           | 36.778736    |\n",
      "|    reward_mean          | 0.23123239   |\n",
      "|    reward_min           | -42.26321    |\n",
      "|    value_loss           | 123          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 458          |\n",
      "|    iterations           | 48           |\n",
      "|    time_elapsed         | 214          |\n",
      "|    total_timesteps      | 98304        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037617153 |\n",
      "|    clip_fraction        | 0.022        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.442       |\n",
      "|    explained_variance   | -0.00611     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 66.1         |\n",
      "|    n_updates            | 470          |\n",
      "|    policy_gradient_loss | -0.00822     |\n",
      "|    reward               | 0.0          |\n",
      "|    reward_max           | 40.866085    |\n",
      "|    reward_mean          | 0.21004096   |\n",
      "|    reward_min           | -49.138138   |\n",
      "|    value_loss           | 199          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 458         |\n",
      "|    iterations           | 49          |\n",
      "|    time_elapsed         | 218         |\n",
      "|    total_timesteps      | 100352      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004890552 |\n",
      "|    clip_fraction        | 0.0311      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.445      |\n",
      "|    explained_variance   | -0.00385    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 85.5        |\n",
      "|    n_updates            | 480         |\n",
      "|    policy_gradient_loss | -0.00736    |\n",
      "|    reward               | 2.2001607   |\n",
      "|    reward_max           | 27.946218   |\n",
      "|    reward_mean          | 0.1910218   |\n",
      "|    reward_min           | -43.710045  |\n",
      "|    value_loss           | 220         |\n",
      "-----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "trained_ppo = agent.train_model(model=model_ppo, \n",
    "                             tb_log_name='ppo',\n",
    "                             total_timesteps=100000) if if_using_ppo else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C6AidlWyvwzm"
   },
   "outputs": [],
   "source": [
    "trained_ppo.save(TRAINED_MODEL_DIR + \"/agent_ppo_\"+env_used) if if_using_ppo else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Zpv4S0-fDBv"
   },
   "source": [
    "### Agent 4: TD3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "JSAHhV4Xc-bh"
   },
   "outputs": [],
   "source": [
    "# agent = DRLAgent(env = env_train)\n",
    "# TD3_PARAMS = {\"batch_size\": 100, \n",
    "#               \"buffer_size\": 1000000, \n",
    "#               \"learning_rate\": 0.001}\n",
    "\n",
    "# model_td3 = agent.get_model(\"td3\",model_kwargs = TD3_PARAMS)\n",
    "\n",
    "# if if_using_td3:\n",
    "#   # set up logger\n",
    "#   tmp_path = RESULTS_DIR + '/td3'\n",
    "#   new_logger_td3 = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "#   # Set new logger\n",
    "#   model_td3.set_logger(new_logger_td3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "OSRxNYAxdKpU"
   },
   "outputs": [],
   "source": [
    "trained_td3 = agent.train_model(model=model_td3, \n",
    "                             tb_log_name='td3',\n",
    "                             total_timesteps=50000) if if_using_td3 else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "OkJV6V_mv2hw"
   },
   "outputs": [],
   "source": [
    "trained_td3.save(TRAINED_MODEL_DIR + \"/agent_td3\") if if_using_td3 else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dr49PotrfG01"
   },
   "source": [
    "### Agent 5: SAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "xwOhVjqRkCdM"
   },
   "outputs": [],
   "source": [
    "# agent = DRLAgent(env = env_train)\n",
    "# SAC_PARAMS = {\n",
    "#     \"batch_size\": 128,\n",
    "#     \"buffer_size\": 100000,\n",
    "#     \"learning_rate\": 0.0001,\n",
    "#     \"learning_starts\": 100,\n",
    "#     \"ent_coef\": \"auto_0.1\",\n",
    "# }\n",
    "\n",
    "# model_sac = agent.get_model(\"sac\",model_kwargs = SAC_PARAMS)\n",
    "\n",
    "# if if_using_sac:\n",
    "#   # set up logger\n",
    "#   tmp_path = RESULTS_DIR + '/sac'\n",
    "#   new_logger_sac = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "#   # Set new logger\n",
    "#   model_sac.set_logger(new_logger_sac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "K8RSdKCckJyH"
   },
   "outputs": [],
   "source": [
    "trained_sac = agent.train_model(model=model_sac, \n",
    "                             tb_log_name='sac',\n",
    "                             total_timesteps=70000) if if_using_sac else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "_SpZoQgPv7GO"
   },
   "outputs": [],
   "source": [
    "trained_sac.save(TRAINED_MODEL_DIR + \"/agent_sac\") if if_using_sac else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PgGm3dQZfRks"
   },
   "source": [
    "## Save the trained agent\n",
    "Trained agents should have already been saved in the \"trained_models\" drectory after you run the code blocks above.\n",
    "\n",
    "For Colab users, the zip files should be at \"./trained_models\" or \"/content/trained_models\".\n",
    "\n",
    "For users running on your local environment, the zip files should be at \"./trained_models\"."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "MRiOtrywfAo1",
    "_gDkU-j-fCmZ",
    "3Zpv4S0-fDBv",
    "Dr49PotrfG01"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
